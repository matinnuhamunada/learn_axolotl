{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14e1ed16-c642-47f4-9632-bbb6c22c58b2",
   "metadata": {},
   "source": [
    "## Getting Started with Axolotl: BGC Analytics with PySpark and Bigslice\n",
    "This Jupyter notebook tutorial is my personal getting started guide to utilizing [Axolotl](https://github.com/JGI-Bioinformatics/axolotl), a scalable distributed genome and metagenome data analysis for the exploration of genomic data, specifically focusing on the identification and study of BGCs, which are groups of genes involved in the biosynthesis of complex bioactive compounds. In this notebook, I am learning how to set up and manage Spark sessions, process GenBank files, and utilize Bigslice applications for efficient and scalable genomic data analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526d0cb-2547-46f0-9874-3f3a70ba3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from axolotl.app.bgc_analysis import BigsliceApp\n",
    "from axolotl.io.genbank import gbkIO\n",
    "from axolotl.data.annotation import RawFeatDF\n",
    "import os, shutil\n",
    "\n",
    "def start_new_spark_session(master_ip, app_name, log_dir=\"../logs\", cores=2, memory=\"2g\", partitions=5):\n",
    "    \"\"\"\n",
    "    This function creates a new Spark session with the specified configuration.\n",
    "    \n",
    "    Parameters:\n",
    "    master_ip (str): The IP address of the master node.\n",
    "    app_name (str): The name of the Spark application.\n",
    "    log_dir (str): The directory where Spark event logs will be stored. Defaults to \"../logs\".\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the SparkSession and SparkContext objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stop the existing SparkContext if there is one\n",
    "    SparkContext.getOrCreate().stop()\n",
    "    \n",
    "    # Create the log directory if it doesn't exist\n",
    "    log_dir = Path(log_dir)\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create a new SparkSession with the specified configuration\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(app_name) \\\n",
    "        .master(master_ip) \\\n",
    "        .config(\"spark.executor.cores\", str(cores)) \\\n",
    "        .config(\"spark.executor.memory\", memory) \\\n",
    "        .config(\"spark.shuffle.useOldFetchProtocol\", True) \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", partitions) \\\n",
    "        .config(\"spark.sql.parquet.enableVectorizedReader\", False) \\\n",
    "        .config(\"spark.eventLog.enabled\", True) \\\n",
    "        .config(\"spark.eventLog.dir\", log_dir) \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Get the SparkContext from the SparkSession\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    # Return the SparkSession and SparkContext\n",
    "    return (spark, sc)\n",
    "\n",
    "def gbk_to_bigslice_app(genbank_table_path, bigslice_app_dir_path, annotations_path):\n",
    "    \"\"\"\n",
    "    This function loads sequence and annotation data from datasets, and creates a BigsliceApp.\n",
    "\n",
    "    Parameters:\n",
    "    genbank_table_path (str): The path to the file containing the datasets.\n",
    "    bigslice_app_dir_path (str): The path to the directory where the BigsliceApp will be created.\n",
    "    annotations_path (str): The path to the file containing the annotations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the path to the file containing the datasets\n",
    "    genbank_table = genbank_table_path\n",
    "\n",
    "    # Load the annotation data from the datasets into a DataFrame\n",
    "    gbkIO.loadSmallFiles2(\n",
    "        str(genbank_table), \n",
    "        df_type=\"annotation\",\n",
    "        minPartitions=5, \n",
    "        skip_malformed_record=True\n",
    "    ).write(\n",
    "        annotations_path, \n",
    "        overwrite=True, \n",
    "        num_partitions=5\n",
    "    )\n",
    "\n",
    "    # Define the path to the directory\n",
    "    bigslice_app_dir = Path(bigslice_app_dir_path)\n",
    "\n",
    "    # Delete the existing directory if it exists\n",
    "    if bigslice_app_dir.exists():\n",
    "        shutil.rmtree(str(bigslice_app_dir))\n",
    "\n",
    "    # Now you can create the BigsliceApp without getting an error\n",
    "    features_df = RawFeatDF.read(annotations_path)\n",
    "    BigsliceApp.create(str(bigslice_app_dir), \n",
    "        features_df, \n",
    "        source_type=\"antismash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d67096-441f-4e4c-9153-d2a40d31cd1c",
   "metadata": {},
   "source": [
    "### Setting up Spark Session & Hadoop Libary (TO DO)\n",
    "To begin our journey with Axolotl and Bigslice for BGC analysis, we first need to initialize a Spark session. This session will serve as the foundation for our distributed data processing tasks. By setting up the Spark session with the local machine as the master node, we enable the utilization of all available cores for parallel processing. Naming our application \"bigslice\" helps in identifying our Spark application processes and logs more easily. The following code snippet demonstrates how to start a new Spark session with these configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f9e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new Spark session with the local machine as the master node and \"bigslice\" as the application name\n",
    "spark, sc = start_new_spark_session(\"local[*]\", \"bigslice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf18663-0fde-4b4c-9817-2d782ee2c3c3",
   "metadata": {},
   "source": [
    "### Setting up input files\n",
    "Before diving into the analysis with Axolotl and Bigslice, it's crucial to gather and prepare our input data. In this case, we're focusing on GenBank (.gbk) files generated by antiSMASH, a popular tool for identifying biosynthetic gene clusters (BGCs) in genomic data. These files contain valuable annotations and sequence information that Axolotl will process.\n",
    "\n",
    "Here's how we prepare our dataset list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the directory containing the datasets\n",
    "dataset_path = \"/home/matinnu/datadrive/vol_atlas_server/datasets/\"\n",
    "\n",
    "# Use a list comprehension to get a list of all .gbk files in the antismash subdirectories\n",
    "datasets = [i for i in Path(dataset_path).glob(\"*/antismash/*region*.gbk\")]\n",
    "\n",
    "# Define the path to the output file\n",
    "outfile = Path(genbank_table)\n",
    "\n",
    "# Create the parent directory of the output file if it doesn't exist\n",
    "outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(outfile, 'w') as f:\n",
    "    # Write the paths of the datasets to the file, one per line\n",
    "    f.writelines(str(dataset) + '\\n' for dataset in datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da772b4a-a048-4e5d-ac2b-2edc6d501f17",
   "metadata": {},
   "source": [
    "### Loading Data into the BiG-SLICE App\n",
    "\n",
    "After preparing our dataset list, the next step involves loading this data into the BiG-SLICE application for analysis. BiG-SLICE is a powerful tool designed for large-scale comparative analysis of biosynthetic gene clusters (BGCs), enabling researchers to uncover the diversity and distribution of BGCs across large genomic datasets. This section outlines the process of converting GenBank files into a format suitable for BiG-SLICE and setting up the necessary directories and databases for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d4a557-7e19-48c2-b1d1-39f72e5ad4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup where to save parquets\n",
    "genbank_table = \"../data/datasets.txt\"\n",
    "bigslice_app_dir = \"../data/interim/bigslice_app\"\n",
    "annotations_dir = \"../data/interim/annotations\"\n",
    "\n",
    "# Setup bigslice HMM database, can be downloaded with hmm\n",
    "bigslice_db_path = Path(os.getenv(\"CONDA_PREFIX\")) / \"bin/bigslice-models\"\n",
    "\n",
    "# Write parquet files to bigslice app\n",
    "gbk_to_bigslice_app(genbank_table, bigslice_app_dir, annotations_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49003be7-6f88-449d-be83-df82a4147537",
   "metadata": {},
   "source": [
    "### Executing BiG-SLICE analysis from the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b4cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigslice_app = BigsliceApp.load(bigslice_app_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c368b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigslice_app.getData(\"bgc\").df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df41d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigslice_app.getData(\"cds\").df.show(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
